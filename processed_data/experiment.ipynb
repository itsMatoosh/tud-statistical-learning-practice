{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T05:22:04.369972Z",
     "start_time": "2025-01-23T05:21:55.224348Z"
    }
   },
   "outputs": [],
   "source": [
    "%pip install facenet-pytorch pandas tqdm scikit-learn opencv-python opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:52:57.052579Z",
     "start_time": "2025-01-23T04:52:57.046722Z"
    }
   },
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "RNG_SEED = 42\n",
    "BATCH_SIZE = 8\n",
    "BOVW_CLUSTERS = 500\n",
    "\n",
    "random.seed(RNG_SEED)\n",
    "torch.manual_seed(RNG_SEED)\n",
    "np.random.seed(RNG_SEED)\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# Check M1 support\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('Running on device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proccess raw images zip into a usable dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # create testing folder\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "    # create label folders\n",
    "    os.makedirs(f\"{DATA_DIR}/face\")\n",
    "    os.makedirs(f\"{DATA_DIR}/no_face\")\n",
    "except:\n",
    "    print(\"Folders already exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "labels = pd.read_csv(\"labels.csv\")\n",
    "\n",
    "# Get all image names\n",
    "\n",
    "files = []\n",
    "labeled_files = set(labels[\"filename\"].values)\n",
    "for (dirpath, dirnames, filenames) in os.walk(DATA_DIR):\n",
    "    files.extend(filenames)\n",
    "    break\n",
    "\n",
    "unmoved = labels[labels[\"filename\"].isin(files)]\n",
    "files = [file for file in files if file not in labeled_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import Image, display, clear_output\n",
    "\n",
    "face_bttn = widgets.Button(description=\"Face\")\n",
    "no_face_bttn = widgets.Button(description=\"No Face\")\n",
    "out = widgets.Output()\n",
    "\n",
    "count = [0]\n",
    "\n",
    "curr_file = ''\n",
    "\n",
    "def face_bttn_clicked(_):\n",
    "    d = {'filename': files[0],\n",
    "                   'label': 'face'}\n",
    "    files.pop(0)\n",
    "    labels.loc[len(labels)] = d\n",
    "\n",
    "    show_widgets()\n",
    "        \n",
    "face_bttn.on_click(face_bttn_clicked)\n",
    "\n",
    "def no_face_clicked(_):\n",
    "    d = {'filename': files[0],\n",
    "                   'label': 'no face'}\n",
    "    files.pop(0)\n",
    "    labels.loc[len(labels)] = d\n",
    "\n",
    "    show_widgets()\n",
    "\n",
    "no_face_bttn.on_click(no_face_clicked)\n",
    "\n",
    "def show_widgets():\n",
    "    clear_output(wait=True)\n",
    "    buttons = widgets.HBox([face_bttn, no_face_bttn])\n",
    "    \n",
    "    image = widgets.Image(\n",
    "        value=Image(filename=f\"/{DATA_DIR}/{files[0]}\").data,\n",
    "        format=\"webp\",\n",
    "        width=300,\n",
    "        height=300\n",
    "    )\n",
    "    \n",
    "    text = widgets.Text(f\"Total labeled: {len(labels)}\")\n",
    "    \n",
    "    display(widgets.VBox([buttons, text, image, out]))\n",
    "    \n",
    "    \n",
    "# show_widgets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_files(row):\n",
    "    filename = row[\"filename\"]\n",
    "    label = row[\"label\"].replace(\" \", \"_\")\n",
    "    \n",
    "    os.rename(f\"data/{filename}\", f\"{DATA_DIR}/{label}/{filename}\")\n",
    "\n",
    "faces = unmoved[unmoved[\"label\"] == 'face']\n",
    "no_faces = unmoved[unmoved[\"label\"] == 'no face']\n",
    "\n",
    "try:\n",
    "    faces.apply(move_files, axis=1)\n",
    "    no_faces.apply(move_files, axis=1)\n",
    "    \n",
    "    print(\"Moved files to relevant folders\")\n",
    "except:\n",
    "    print(\"Images are already moved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((160, 160)),\n",
    "])\n",
    "\n",
    "tensor_transform = transforms.Compose([\n",
    "    transforms.Resize((160, 160)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    return list(images), list(labels)\n",
    "\n",
    "base_train = datasets.ImageFolder(f\"{DATA_DIR}/train\", transform=transform)\n",
    "base_test = datasets.ImageFolder(f\"{DATA_DIR}/test\", transform=transform)\n",
    "tensor_train = datasets.ImageFolder(f\"{DATA_DIR}/train\", transform=tensor_transform)\n",
    "tensor_test = datasets.ImageFolder(f\"{DATA_DIR}/test\", transform=tensor_transform)\n",
    "\n",
    "base_loader_train = DataLoader(base_train, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=True)\n",
    "base_loader_test = DataLoader(base_test, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=True)\n",
    "tensor_loader_train = DataLoader(tensor_train, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=True)\n",
    "tensor_loader_test = DataLoader(tensor_test, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define MTCNN baseline\n",
    "We use the default params for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T03:57:49.144219Z",
     "start_time": "2025-01-23T03:57:48.987251Z"
    }
   },
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(\n",
    "    image_size=160, margin=0, min_face_size=20,\n",
    "    thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True,\n",
    "    keep_all=True, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing MTCNN accuracy with manually labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T22:07:59.251191Z",
     "start_time": "2025-01-22T21:45:42.716847Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "for X, Y in tqdm(base_loader_test):\n",
    "    for i in range(0, len(X)):\n",
    "        x = X[i]\n",
    "        y = Y[i]\n",
    "\n",
    "        x_aligned, probs = mtcnn(x, return_prob=True)\n",
    "\n",
    "        y_p = 0 if x_aligned is not None else 1\n",
    "        \n",
    "        y_pred.append(y_p)\n",
    "    y_true.extend(Y)               \n",
    "\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training SVM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T00:39:45.714401Z",
     "start_time": "2025-01-23T00:39:45.711069Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_data(loader):\n",
    "    X = []\n",
    "    y = []\n",
    "    for images, labels in tqdm(loader, desc=\"Flattening data\"):\n",
    "        # Convert images to numpy arrays and flatten\n",
    "        images_flat = [np.array(img).flatten() for img in images]\n",
    "        X.extend(images_flat)\n",
    "        y.extend(labels)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T00:47:47.283427Z",
     "start_time": "2025-01-23T00:39:47.780585Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train = preprocess_data(base_loader_train)\n",
    "X_test, y_test = preprocess_data(base_loader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # Perform PCA \n",
    "pca = PCA(n_components=50)  \n",
    "X_train_pca = pca.fit_transform(X_train_scaled) \n",
    "X_test_pca = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance_ratio = np.array(pca.explained_variance_ratio_)\n",
    "\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "cumulative_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM Model \n",
    "svm_model = svm.SVC(kernel=\"rbf\",verbose=True)\n",
    "svm_model.fit(X_train_pca, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm_model.predict(X_test_pca)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get SIFT features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T07:53:17.409749Z",
     "start_time": "2025-01-23T07:53:17.407614Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Create SIFT extractor\n",
    "sift = cv2.SIFT_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T07:53:19.604948Z",
     "start_time": "2025-01-23T07:53:19.600228Z"
    }
   },
   "outputs": [],
   "source": [
    "def tensor_to_opencv_img(tensor_img):\n",
    "    \"\"\"\n",
    "    Convert a single image from a PyTorch tensor (C,H,W) to a NumPy array (H,W) or (H,W,3).\n",
    "    We'll convert to grayscale for SIFT.\n",
    "    \"\"\"\n",
    "    # tensor_img shape: (3, H, W) if color\n",
    "    # Move to CPU, convert to numpy\n",
    "    img_np = tensor_img.cpu().numpy()\n",
    "\n",
    "    # img_np shape is (3, H, W). We can convert to (H, W, 3) by transposing\n",
    "    img_np = np.transpose(img_np, (1, 2, 0))  # (H, W, 3)\n",
    "\n",
    "    # Convert to uint8 [0..255] if necessary\n",
    "    img_np = (img_np * 255.0).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n",
    "    return gray\n",
    "\n",
    "def extract_descriptors_from_dataloader(dataloader):\n",
    "    \"\"\"\n",
    "    Loop through an entire DataLoader, extract SIFT descriptors for each image.\n",
    "    \"\"\"\n",
    "    descriptors_per_image = []\n",
    "    labels_list = []\n",
    "\n",
    "    # Assume we already have train_loader that yields (images, labels)\n",
    "    for images, labels in tqdm(dataloader):\n",
    "        # images shape: (batch_size, 3, H, W)\n",
    "        # labels shape: (batch_size,)\n",
    "        \n",
    "        batch_size = len(images)\n",
    "        for i in range(batch_size):\n",
    "            # Convert one image to grayscale OpenCV format\n",
    "            gray_img = tensor_to_opencv_img(images[i])\n",
    "            # Extract SIFT descriptors\n",
    "            kp, desc = sift.detectAndCompute(gray_img, None)\n",
    "            if desc is not None:\n",
    "                descriptors_per_image.append(desc)\n",
    "            else:\n",
    "                # Some images might have no descriptors\n",
    "                descriptors_per_image.append(np.zeros((0,128), dtype=np.float32))\n",
    "\n",
    "            # We also keep the label so we can match it up later\n",
    "            labels_list.append(labels[i])\n",
    "\n",
    "    return descriptors_per_image, labels_list\n",
    "\n",
    "def build_bovw_histogram(descriptors, kmeans_model):\n",
    "    \"\"\"\n",
    "    Given SIFT descriptors (num_keypoints,128) for ONE image,\n",
    "    assign each descriptor to the nearest cluster and build a histogram of size BOVW_CLUSTERS.\n",
    "    \"\"\"\n",
    "    hist = np.zeros((BOVW_CLUSTERS), dtype=np.float32)\n",
    "    if descriptors is None or len(descriptors) == 0:\n",
    "        return hist  # no keypoints => zero histogram\n",
    "\n",
    "    words = kmeans_model.predict(descriptors)\n",
    "    for w in words:\n",
    "        hist[w] += 1\n",
    "\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T07:59:02.924200Z",
     "start_time": "2025-01-23T07:53:25.203785Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Collecting SIFT descriptors from train_loader...\")\n",
    "all_descriptors, all_labels = extract_descriptors_from_dataloader(tensor_loader_train)\n",
    "print(f\"Collected descriptors from {len(all_descriptors)} training images.\")\n",
    "\n",
    "# Stack all descriptors into one large array for K-Means (excluding empty ones)\n",
    "desc_nonempty = [d for d in all_descriptors if d.shape[0] > 0]\n",
    "if len(desc_nonempty) > 0:\n",
    "    all_train_desc = np.vstack(desc_nonempty)\n",
    "else:\n",
    "    all_train_desc = np.zeros((0, 128), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T08:30:54.434707Z",
     "start_time": "2025-01-23T08:00:10.680808Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "if all_train_desc.shape[0] == 0:\n",
    "    print(\"No descriptors found in training set! Can't build K-Means.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Running K-Means on {all_train_desc.shape[0]} descriptors with {BOVW_CLUSTERS} clusters...\")\n",
    "kmeans = KMeans(n_clusters=BOVW_CLUSTERS, random_state=RNG_SEED, verbose=1)\n",
    "kmeans.fit(all_train_desc)\n",
    "print(\"K-Means done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T09:02:56.536467Z",
     "start_time": "2025-01-23T09:02:02.066014Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "train_histograms = []\n",
    "train_labels = []\n",
    "\n",
    "idx = 0\n",
    "print(\"Building BoVW histograms for training set...\")\n",
    "for desc in tqdm(all_descriptors):\n",
    "    hist = build_bovw_histogram(desc, kmeans)\n",
    "    train_histograms.append(hist)\n",
    "    train_labels.append(all_labels[idx])\n",
    "    idx += 1\n",
    "\n",
    "train_histograms = np.array(train_histograms, dtype=np.float32)\n",
    "train_labels = np.array(train_labels, dtype=np.int64)\n",
    "\n",
    "# (Optional) Normalize histograms\n",
    "train_histograms = normalize(train_histograms, norm='l2', axis=1)\n",
    "\n",
    "print(\"Train BoVW shape:\", train_histograms.shape)  # (num_train_images, NUM_CLUSTERS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T09:07:46.696297Z",
     "start_time": "2025-01-23T09:07:45.744720Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"Training Logistic Regression on BoVW histograms...\")\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(train_histograms, train_labels)\n",
    "print(\"Logistic Regression training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T09:09:31.605747Z",
     "start_time": "2025-01-23T09:07:50.581495Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Extracting SIFT descriptors from test_loader...\")\n",
    "test_descriptors_list, test_labels_list = extract_descriptors_from_dataloader(tensor_loader_test)\n",
    "\n",
    "print(\"Building BoVW histograms for the test set...\")\n",
    "test_histograms = []\n",
    "for desc in tqdm(test_descriptors_list):\n",
    "    hist = build_bovw_histogram(desc, kmeans)\n",
    "    test_histograms.append(hist)\n",
    "\n",
    "test_histograms = np.array(test_histograms, dtype=np.float32)\n",
    "test_histograms = normalize(test_histograms, norm='l2', axis=1)\n",
    "test_labels = np.array(test_labels_list, dtype=np.int64)\n",
    "\n",
    "print(\"Predicting on test histograms...\")\n",
    "test_preds = clf.predict(test_histograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T09:09:35.567164Z",
     "start_time": "2025-01-23T09:09:35.560720Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(test_labels, test_preds)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Gaussian Mixture model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "for images, labels in tqdm(tensor_loader_train):\n",
    "    # Flatten images to shape\n",
    "    images_flat = [img.numpy().transpose(1, 2, 0).flatten() for img in images]\n",
    "    X.extend(images_flat)\n",
    "    y.extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y = np.asarray(y, dtype=int)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(train_histograms, train_labels, test_size=0.3, random_state=42)\n",
    "\n",
    "n_classes = len(np.unique(y))\n",
    "gmm_models = []\n",
    "gmm_models_sift = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training with SIFT\")\n",
    "for label in tqdm(range(n_classes)):\n",
    "    X_class = train_histograms[train_labels == label]\n",
    "    \n",
    "    gmm = GaussianMixture(n_components=n_classes, covariance_type='full', random_state=42)\n",
    "    gmm.fit(X_class)\n",
    "    gmm_models_sift.append(gmm)\n",
    "\n",
    "print(\"Training without SIFT\")\n",
    "for label in tqdm(range(n_classes)):\n",
    "    X_class = X_scaled[y == label]\n",
    "    \n",
    "    gmm = GaussianMixture(n_components=n_classes, covariance_type='full', random_state=42)\n",
    "    gmm.fit(X_class)\n",
    "    gmm_models.append(gmm)\n",
    "\n",
    "\n",
    "\n",
    "# gmm = GaussianMixture(n_components=len(idx_to_class), random_state=42)\n",
    "# gmm.fit(X_train)\n",
    "\n",
    "# # Step 5: Predict Labels\n",
    "# y_pred = gmm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "\n",
    "for sample in tqdm(X_test):\n",
    "    likelihoods = gmm.score_samples(sample.reshape(1, -1))\n",
    "    \n",
    "    y_pred.append(np.argmax(likelihoods))\n",
    "    \n",
    "    \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "accuracy\n",
    "\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
