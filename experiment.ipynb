{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T05:22:04.369972Z",
     "start_time": "2025-01-23T05:21:55.224348Z"
    }
   },
   "outputs": [],
   "source": [
    "%pip install facenet-pytorch pandas tqdm scikit-learn opencv-python opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:52:57.052579Z",
     "start_time": "2025-01-23T04:52:57.046722Z"
    }
   },
   "outputs": [],
   "source": [
    "# Misc\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt \n",
    "from IPython.display import Image, display, clear_output\n",
    "\n",
    "# MTCNN and relevant packages\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from facenet_pytorch import MTCNN\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Statistical models and relevant packages\n",
    "from sklearn.svm import SVC\n",
    "from scipy.stats import mode\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.base import BaseEstimator, ClusterMixin\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "RNG_SEED = 42\n",
    "BATCH_SIZE = 8\n",
    "BOVW_CLUSTERS = 500\n",
    "\n",
    "random.seed(RNG_SEED)\n",
    "torch.manual_seed(RNG_SEED)\n",
    "np.random.seed(RNG_SEED)\n",
    "\n",
    "# Check M1 support\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('Running on device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proccess raw images zip into a usable dataset\n",
    "\n",
    "### DISCLAIMER\n",
    "The following code is shown for posterity, but might not function as intend, as all images have been labeled and the full dataset is now on GitHub. Run at your own risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # create testing folder\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "    # create label folders\n",
    "    os.makedirs(f\"{DATA_DIR}/face\")\n",
    "    os.makedirs(f\"{DATA_DIR}/no_face\")\n",
    "except:\n",
    "    print(\"Folders already exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\"labels.csv\")\n",
    "\n",
    "# Get all image names\n",
    "\n",
    "files = []\n",
    "labeled_files = set(labels[\"filename\"].values)\n",
    "for (dirpath, dirnames, filenames) in os.walk(DATA_DIR):\n",
    "    files.extend(filenames)\n",
    "    break\n",
    "\n",
    "unmoved = labels[labels[\"filename\"].isin(files)]\n",
    "files = [file for file in files if file not in labeled_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_bttn = widgets.Button(description=\"Face\")\n",
    "no_face_bttn = widgets.Button(description=\"No Face\")\n",
    "out = widgets.Output()\n",
    "\n",
    "count = [0]\n",
    "\n",
    "curr_file = ''\n",
    "\n",
    "def face_bttn_clicked(_):\n",
    "    d = {'filename': files[0],\n",
    "                   'label': 'face'}\n",
    "    files.pop(0)\n",
    "    labels.loc[len(labels)] = d\n",
    "\n",
    "    show_widgets()\n",
    "        \n",
    "face_bttn.on_click(face_bttn_clicked)\n",
    "\n",
    "def no_face_clicked(_):\n",
    "    d = {'filename': files[0],\n",
    "                   'label': 'no face'}\n",
    "    files.pop(0)\n",
    "    labels.loc[len(labels)] = d\n",
    "\n",
    "    show_widgets()\n",
    "\n",
    "no_face_bttn.on_click(no_face_clicked)\n",
    "\n",
    "def show_widgets():\n",
    "    clear_output(wait=True)\n",
    "    buttons = widgets.HBox([face_bttn, no_face_bttn])\n",
    "    \n",
    "    image = widgets.Image(\n",
    "        value=Image(filename=f\"/{DATA_DIR}/{files[0]}\").data,\n",
    "        format=\"webp\",\n",
    "        width=300,\n",
    "        height=300\n",
    "    )\n",
    "    \n",
    "    text = widgets.Text(f\"Total labeled: {len(labels)}\")\n",
    "    \n",
    "    display(widgets.VBox([buttons, text, image, out]))\n",
    "    \n",
    "    \n",
    "# !!!DISCLAIMER!!!\n",
    "# This line throws an error at the moment because there are no (accessable) unlabeled images.\n",
    "# show_widgets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_files(row):\n",
    "    filename = row[\"filename\"]\n",
    "    label = row[\"label\"].replace(\" \", \"_\")\n",
    "    \n",
    "    os.rename(f\"data/{filename}\", f\"{DATA_DIR}/{label}/{filename}\")\n",
    "\n",
    "faces = unmoved[unmoved[\"label\"] == 'face']\n",
    "no_faces = unmoved[unmoved[\"label\"] == 'no face']\n",
    "\n",
    "try:\n",
    "    faces.apply(move_files, axis=1)\n",
    "    no_faces.apply(move_files, axis=1)\n",
    "    \n",
    "    print(\"Moved files to relevant folders\")\n",
    "except:\n",
    "    print(\"Images are already moved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((160, 160)),\n",
    "])\n",
    "\n",
    "tensor_transform = transforms.Compose([\n",
    "    transforms.Resize((160, 160)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    return list(images), list(labels)\n",
    "\n",
    "base_train = datasets.ImageFolder(f\"{DATA_DIR}/train\", transform=transform)\n",
    "base_test = datasets.ImageFolder(f\"{DATA_DIR}/test\", transform=transform)\n",
    "tensor_train = datasets.ImageFolder(f\"{DATA_DIR}/train\", transform=tensor_transform)\n",
    "tensor_test = datasets.ImageFolder(f\"{DATA_DIR}/test\", transform=tensor_transform)\n",
    "\n",
    "base_loader_train = DataLoader(base_train, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=True)\n",
    "base_loader_test = DataLoader(base_test, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=True)\n",
    "tensor_loader_train = DataLoader(tensor_train, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=True)\n",
    "tensor_loader_test = DataLoader(tensor_test, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell was ran multiple times to get an equal distribution of examples from both classes\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(8, 6))\n",
    "\n",
    "n = 10\n",
    "i = 0\n",
    "\n",
    "axes = axes.flatten()\n",
    "ids = random.sample(range(len(base_test) + 1), len(axes))\n",
    "images = [base_test[i] for i in ids]\n",
    "\n",
    "for ax, (img, label) in zip(axes, images):\n",
    "    \n",
    "    ax.imshow(img)\n",
    "    ax.set_title('face' if label == 0 else 'no face')\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define MTCNN baseline\n",
    "We use the default params for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T03:57:49.144219Z",
     "start_time": "2025-01-23T03:57:48.987251Z"
    }
   },
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(\n",
    "    image_size=160, margin=0, min_face_size=20,\n",
    "    thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True,\n",
    "    keep_all=True, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing MTCNN accuracy with manually labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T22:07:59.251191Z",
     "start_time": "2025-01-22T21:45:42.716847Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "for X, Y in tqdm(base_loader_test):\n",
    "    for i in range(0, len(X)):\n",
    "        x = X[i]\n",
    "        y = Y[i]\n",
    "\n",
    "        x_aligned, probs = mtcnn(x, return_prob=True)\n",
    "\n",
    "        y_p = 0 if x_aligned is not None else 1\n",
    "        \n",
    "        y_pred.append(y_p)\n",
    "    y_true.extend(Y)               \n",
    "\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "You can run the following cells to generate the necessary data or use the precomputed data provided in `/processed_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(loader):\n",
    "    X = []\n",
    "    y = []\n",
    "    for images, labels in tqdm(loader, desc=\"Flattening data\"):\n",
    "        # Convert images to numpy arrays and flatten\n",
    "        images_flat = [np.array(img).flatten() for img in images]\n",
    "        X.extend(images_flat)\n",
    "        y.extend(labels)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train_flat, y_train = preprocess_data(base_loader_train)\n",
    "X_test_flat, y_test = preprocess_data(base_loader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train_flat)\n",
    "X_test_scaled = scaler.fit_transform(X_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=50)\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.fit_transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance_ratio = np.array(pca.explained_variance_ratio_)\n",
    "\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "cumulative_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T07:53:17.409749Z",
     "start_time": "2025-01-23T07:53:17.407614Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Create SIFT extractor\n",
    "sift = cv2.SIFT_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T07:53:19.604948Z",
     "start_time": "2025-01-23T07:53:19.600228Z"
    }
   },
   "outputs": [],
   "source": [
    "def tensor_to_opencv_img(tensor_img):\n",
    "    \"\"\"\n",
    "    Convert a single image from a PyTorch tensor (C,H,W) to a NumPy array (H,W) or (H,W,3).\n",
    "    We'll convert to grayscale for SIFT.\n",
    "    \"\"\"\n",
    "    # tensor_img shape: (3, H, W) if color\n",
    "    # Move to CPU, convert to numpy\n",
    "    img_np = tensor_img.cpu().numpy()\n",
    "\n",
    "    # img_np shape is (3, H, W). We can convert to (H, W, 3) by transposing\n",
    "    img_np = np.transpose(img_np, (1, 2, 0))  # (H, W, 3)\n",
    "\n",
    "    # Convert to uint8 [0..255] if necessary\n",
    "    img_np = (img_np * 255.0).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n",
    "    return gray\n",
    "\n",
    "def extract_descriptors_from_dataloader(dataloader):\n",
    "    \"\"\"\n",
    "    Loop through an entire DataLoader, extract SIFT descriptors for each image.\n",
    "    \"\"\"\n",
    "    descriptors_per_image = []\n",
    "    labels_list = []\n",
    "\n",
    "    # Assume we already have train_loader that yields (images, labels)\n",
    "    for images, labels in tqdm(dataloader):\n",
    "        # images shape: (batch_size, 3, H, W)\n",
    "        # labels shape: (batch_size,)\n",
    "        \n",
    "        batch_size = len(images)\n",
    "        for i in range(batch_size):\n",
    "            # Convert one image to grayscale OpenCV format\n",
    "            gray_img = tensor_to_opencv_img(images[i])\n",
    "            # Extract SIFT descriptors\n",
    "            kp, desc = sift.detectAndCompute(gray_img, None)\n",
    "            if desc is not None:\n",
    "                descriptors_per_image.append(desc)\n",
    "            else:\n",
    "                # Some images might have no descriptors\n",
    "                descriptors_per_image.append(np.zeros((0,128), dtype=np.float32))\n",
    "\n",
    "            # We also keep the label so we can match it up later\n",
    "            labels_list.append(labels[i])\n",
    "\n",
    "    return descriptors_per_image, labels_list\n",
    "\n",
    "def build_bovw_histogram(descriptors, kmeans_model):\n",
    "    \"\"\"\n",
    "    Given SIFT descriptors (num_keypoints,128) for ONE image,\n",
    "    assign each descriptor to the nearest cluster and build a histogram of size BOVW_CLUSTERS.\n",
    "    \"\"\"\n",
    "    hist = np.zeros((BOVW_CLUSTERS), dtype=np.float32)\n",
    "    if descriptors is None or len(descriptors) == 0:\n",
    "        return hist  # no keypoints => zero histogram\n",
    "\n",
    "    words = kmeans_model.predict(descriptors)\n",
    "    for w in words:\n",
    "        hist[w] += 1\n",
    "\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T07:59:02.924200Z",
     "start_time": "2025-01-23T07:53:25.203785Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Collecting SIFT descriptors from train_loader...\")\n",
    "all_descriptors, all_labels = extract_descriptors_from_dataloader(tensor_loader_train)\n",
    "print(f\"Collected descriptors from {len(all_descriptors)} training images.\")\n",
    "\n",
    "# Stack all descriptors into one large array for K-Means (excluding empty ones)\n",
    "desc_nonempty = [d for d in all_descriptors if d.shape[0] > 0]\n",
    "if len(desc_nonempty) > 0:\n",
    "    all_train_desc = np.vstack(desc_nonempty)\n",
    "else:\n",
    "    all_train_desc = np.zeros((0, 128), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T08:30:54.434707Z",
     "start_time": "2025-01-23T08:00:10.680808Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "if all_train_desc.shape[0] == 0:\n",
    "    print(\"No descriptors found in training set! Can't build K-Means.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Running K-Means on {all_train_desc.shape[0]} descriptors with {BOVW_CLUSTERS} clusters...\")\n",
    "kmeans = KMeans(n_clusters=BOVW_CLUSTERS, random_state=RNG_SEED, verbose=1)\n",
    "kmeans.fit(all_train_desc)\n",
    "print(\"K-Means done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T09:02:56.536467Z",
     "start_time": "2025-01-23T09:02:02.066014Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "X_train_sift = []\n",
    "y_train = []\n",
    "\n",
    "idx = 0\n",
    "print(\"Building BoVW histograms for training set...\")\n",
    "for desc in tqdm(all_descriptors):\n",
    "    hist = build_bovw_histogram(desc, kmeans)\n",
    "    X_train_sift.append(hist)\n",
    "    y_train.append(all_labels[idx])\n",
    "    idx += 1\n",
    "\n",
    "X_train_sift = np.array(X_train_sift, dtype=np.float32)\n",
    "y_train = np.array(y_train, dtype=np.int64)\n",
    "X_train_sift = normalize(X_train_sift, norm='l2', axis=1)\n",
    "\n",
    "print(\"Train BoVW shape:\", X_train_sift.shape)  # (num_train_images, NUM_CLUSTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting SIFT descriptors from test_loader...\")\n",
    "test_descriptors_list, y_train_sift = extract_descriptors_from_dataloader(tensor_loader_test)\n",
    "\n",
    "print(\"Building BoVW histograms for the test set...\")\n",
    "X_test_sift = []\n",
    "for desc in tqdm(test_descriptors_list):\n",
    "    hist = build_bovw_histogram(desc, kmeans)\n",
    "    X_test_sift.append(hist)\n",
    "\n",
    "X_test_sift = np.array(X_test_sift, dtype=np.float32)\n",
    "X_test_sift = normalize(X_test_sift, norm='l2', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMMClassifier(ClusterMixin, BaseEstimator):\n",
    "    def __init__(self, n_components=2, covariance_type='full', init_params='kmeans', reg_covar=1e-06, random_state=42):\n",
    "        self.n_components = n_components\n",
    "        self.covariance_type = covariance_type\n",
    "        self.init_params = init_params\n",
    "        self.reg_covar = reg_covar\n",
    "        \n",
    "        self.random_state = random_state\n",
    "\n",
    "        self.gmm = GaussianMixture(n_components=n_components, covariance_type=covariance_type, random_state=random_state)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.gmm.fit(X)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = self.gmm.predict(X)\n",
    "        return y_pred\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        mapped_preds = np.zeros_like(y)\n",
    "\n",
    "        for cluster in range(self.n_components):\n",
    "            mask = y_pred == cluster\n",
    "            if np.any(mask):\n",
    "                mapped_preds[mask] = mode(y[mask])[0]\n",
    "\n",
    "        return accuracy_score(y, mapped_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(make_model):\n",
    "    \"\"\"\n",
    "    Runs experiment on model with provided initialization function.\n",
    "    \n",
    "    Inputs:\n",
    "    - make_model: function returning appropriate model\n",
    "    \"\"\"\n",
    "    \n",
    "    features = ['pca', 'sift']\n",
    "    \n",
    "    y_train = np.load(\"processed_data/y_train.npy\")\n",
    "    y_test = np.load(\"processed_data/y_test.npy\")\n",
    "    \n",
    "    print(\"Starting auto-generated data experiment\")\n",
    "\n",
    "    for feature in features:\n",
    "        with open(f\"processed_data/X_train_{feature}.npy\", \"rb\") as f:\n",
    "            X = np.load(f)\n",
    "\n",
    "        print(f\"Training with {feature} features with shape: {X.shape}\")\n",
    "        model = make_model()\n",
    "        scores = cross_val_score(model, X, y_train, cv=5, n_jobs=-1, scoring='accuracy', verbose=1)\n",
    "        print(f\"Finished Cross-validation with with {feature} features.\\nCross-val Accuracy: {scores.mean():.2f} ({scores.std():.2f} std)\")\n",
    "\n",
    "        with open(f\"processed_data/X_test_{feature}.npy\", \"rb\") as f:\n",
    "            X_test = np.load(f)\n",
    "\n",
    "        model.fit(X, y_train)\n",
    "        accuracy = model.score(X_test, y_test)\n",
    "        print(f\"Accuracy against manual labels: {accuracy:.2f}\")\n",
    "        \n",
    "    \n",
    "    print(\"Starting manual data experiment\")\n",
    "    for feature in features:\n",
    "        with open(f\"processed_data/X_test_{feature}.npy\", \"rb\") as f:\n",
    "            X = np.load(f)\n",
    "\n",
    "        print(f\"Training with {feature} features with shape: {X.shape}\")\n",
    "        model = make_model()\n",
    "        scores = cross_val_score(model, X, y_test, cv=5, n_jobs=-1, scoring='accuracy', verbose=1)\n",
    "\n",
    "        print(f\"Finished Cross-validation with with {feature} features.\\nCross-val Accuracy: {scores.mean():.2f} ({scores.std():.2f} std)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_logistic():\n",
    "    return LogisticRegression(random_state=RNG_SEED)\n",
    "\n",
    "def make_svm():\n",
    "    return SVC(random_state=RNG_SEED)\n",
    "\n",
    "def make_gaussian():\n",
    "    return GMMClassifier(random_state=RNG_SEED)\n",
    "\n",
    "print(\"Running logistic\")\n",
    "run_experiment(make_logistic)\n",
    "print(\"Running SVM\")\n",
    "run_experiment(make_svm)\n",
    "print(\"Running Gaussian\")\n",
    "run_experiment(make_gaussian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.load(\"processed_data/y_train.npy\")\n",
    "y_test = np.load(\"processed_data/y_test.npy\") \n",
    "estimators = [\n",
    "    ('logi', make_logistic(), \"pca\"),\n",
    "    # ('svm', make_svm(), \"pca\"),\n",
    "    ('gauss', make_gaussian(), \"sift\"),\n",
    "]\n",
    "\n",
    "parameters = [\n",
    "    # Logi\n",
    "    {\n",
    "        \"penalty\": [None, \"l2\", \"l1\", \"elasticnet\"],\n",
    "        \"C\": [0.25, 0.50, 0.75, 1.0],\n",
    "        \"fit_intercept\": [True, False],\n",
    "        \"l1_ratio\": [0.0, 0.25, 0.50, 0.75, 1.0],\n",
    "        \"tol\": [1e-03, 1e-04, 1e-05],\n",
    "    },\n",
    "    # SVM\n",
    "    {},\n",
    "    # Gauss\n",
    "    {\n",
    "        \"covariance_type\": [\"full\", \"tied\", \"diag\", \"spherical\"],\n",
    "        \"init_params\": [\"kmeans\", \"k-means++\", \"random\", \"random_from_data\"],\n",
    "        \"reg_covar\": [1e-07, 1e-06, 1e-05, 1e-04],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (name, estimator, feature), params in zip(estimators, parameters):\n",
    "    print(f\"Running grid search for {name} with {feature}\")\n",
    "    \n",
    "    X = np.load(f\"processed_data/X_train_{feature}.npy\")\n",
    "    clf = GridSearchCV(estimator, params, n_jobs=-1)\n",
    "    \n",
    "    clf.fit(X, y)\n",
    "    \n",
    "    print(f\"Best params: {clf.best_params_}\\n Best score: {clf.best_score_}\")\n",
    "    \n",
    "    with open(f'{name}-grid_search.pkl', 'wb') as f:\n",
    "        pickle.dump(clf, f)\n",
    "        \n",
    "    # Get manual accuracy\n",
    "    X = np.load(f\"processed_data/X_test_{feature}.npy\")\n",
    "    \n",
    "    estimator = clf.best_estimator_\n",
    "    accuracy = estimator.score(X, y_test)\n",
    "    \n",
    "    print(f\"Accuracy on manual set with optimal parameters: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [('results/logi-grid_search.pkl', 'processed_data/X_test_pca.npy'), ('results/gauss-grid_search.pkl', 'processed_data/X_test_sift.npy')]\n",
    "\n",
    "y = np.load(\"processed_data/y_test.npy\")\n",
    "for obj, dataset in estimators:\n",
    "    print(f\"Using {obj}\")\n",
    "    \n",
    "    with open(obj, \"rb\") as f:\n",
    "        clf: GridSearchCV = pickle.load(f)\n",
    "        \n",
    "    with open(dataset, \"rb\") as f:\n",
    "        X = np.load(f)\n",
    "\n",
    "    estimator = clf.best_estimator_\n",
    "    correct = 0\n",
    "    times = []\n",
    "    \n",
    "    for image, label in zip(X, y):\n",
    "        start = time.time()\n",
    "        pred = estimator.predict([image])\n",
    "        end = time.time()\n",
    "        \n",
    "        times.append(end - start)\n",
    "        \n",
    "        if pred[0] == label:\n",
    "            correct += 1\n",
    "    \n",
    "    print(f\"Accuracy: {correct / len(y)} \\n Time (in milliseconds) mean: {np.mean(times) * 1000} ({np.std(times) * 1000} std) \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/gauss-grid_search.pkl\", \"rb\") as f:\n",
    "    clf: GridSearchCV = pickle.load(f)\n",
    "    \n",
    "data = pd.DataFrame(clf.cv_results_)\n",
    "\n",
    "data.loc[data[\"params\"] == clf.best_params_]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
